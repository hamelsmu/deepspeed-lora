{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80ce8a3-4af5-4319-9fd7-3b056f155524",
   "metadata": {},
   "source": [
    "This notebook follows [this blog post](https://www.philschmid.de/deepspeed-lora-flash-attention) from Phillip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c3359-96fc-4cf8-bae1-cb853d8d45f0",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1346b76-15b4-44f9-ba2a-52a1709e22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d695f99-f0f7-4d42-a2b9-bdd02e2e2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_ENTITY\"] = \"hamelsmu\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"deepspeed-bench\" # log to your project \n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"all\" # log your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a829c650-4b3b-4a70-a628-072fa125bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'What kind of dog should I get?', 'context': '', 'response': 'There are many dog breeds to choose from. Choosing a dog breed is a personal choice. Consider what kind of lifestyle you live and pick a dog that fits your lifestyle. For example, if you are allergic to dogs you may consider a poodle, or poodle mix as they tend to be hypoallergenic.', 'category': 'general_qa'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", \n",
    "                       split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce68baeb-d0e6-45d4-a7ac-1cce6680fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b52bf26-6e74-4ac1-8b8c-8015d1d72619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is buoyant force?\n",
      "\n",
      "### Answer\n",
      "The upward force exerted on a body, partially or fully immersed in a fluid, is known as buoyant force. This upward force is also called Upthrust. This is related  to the Archimedes principle. If an object is partially or fully submerged in any fluid, the upward force and the fluid displaced is equal to the upward force exerted by the fluid.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a58065-1b1a-4eab-b2dc-6d3debb45be5",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29f1eb1d-e57a-4903-a68b-9018c5118ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd9b5854-3513-4a79-964f-ab68f93670d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "863e212f-0c29-420a-8100-f1405d14ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is combinatorial optimisation?\n",
      "\n",
      "### Answer\n",
      "Combinatorial optimisation is a field of applied mathematics, combining techniques from combinatorics, linear programming, and the theory of algorithms, to solve discrete optimisation problems. It is usually used as an alias of discrete optimisation. A combinatorial optimisation problem can generally be drawn as a triple (S, f, C), where S is a given search space, f is the objective function, which should be either maximised or minimised, and C is the set of constraints that have to be fulfilled to obtain feasible solutions. The goal is to find a globally optimal solution, meaning a solution s' that belongs to S, with either the highest or lowest objective value in the case of maximisation or minimisation, each under the restriction of constraints.</s>\n"
     ]
    }
   ],
   "source": [
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5eabe0d1-f593-4c94-a05e-c51dce3320ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf7cd2-017c-4ea7-ad3e-08c9fd9971f2",
   "metadata": {},
   "source": [
    "I changed `chunk_length` below to get the desired sequence length I wanted.  I had two versions: \n",
    "\n",
    "1. this `2048` version from Phillips blog post, which is named `dolly-processed`\n",
    "2. and a really small one that was `64`, which I cut off at `3200` examples for to keep it extra small.  I named this data `dolly-processed-tiny-truncated`.  I used this small version for the situation where I was comparing bs=1 with bs=200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "054cc4de-fe38-402f-aa51-a86c6a1f0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4bc7b19-8320-41a8-b4f8-72e844897de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0f8ab13a-2b7a-4be4-9cdb-81f135023223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721b5d9-e8cd-4b25-a91c-4c39e17b7582",
   "metadata": {},
   "source": [
    "### What is going on here?\n",
    "\n",
    "`chunk` is packing examples into one contiguous \"row\" that is of sequence length=2048, the remainder get's put into the next \"batch\" such that there is effectively no padding.  Basically we are cramming as much data through the model as possible.  \n",
    "\n",
    "I can imagine this is how pre-training works more generally, but I am not sure this is something that makes sense for instruction-tuning?  The reason is that in practice the model isn't going to see examples that are like this, so should we really be instruction tuning this way?  (I DM'd Phillip on Twitter to ask him his opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "211da60f-2738-41c3-a618-121d730fc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq len of 1st example: 2048\n"
     ]
    }
   ],
   "source": [
    "print(f\"seq len of 1st example: {len(lm_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc6b972d-cc29-413f-89c2-ee0b4d751db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq len of 2nd example: 2048\n"
     ]
    }
   ],
   "source": [
    "print(f\"seq len of 2nd example: {len(lm_dataset[1]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcdeef09-cbe5-46d4-8a51-fea108624b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27ca72-ad70-4511-bb7e-2f131001f6bd",
   "metadata": {},
   "source": [
    "### Save Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f07278c-b490-41a3-bf42-085f06c6f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████| 1581/1581 [00:00<00:00, 28107.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset.save_to_disk('dolly-processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbd1f2-dbaa-4e49-b08c-173707d33476",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Look at the overview tab of runs in [this project](https://wandb.ai/hamelsmu/deepspeed-bench?workspace=user-hamelsmu) to see (most) of the CLI command used to run each go of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "289638cf-f8bb-401f-8b12-c227ec717567",
   "metadata": {},
   "outputs": [],
   "source": [
    "!WANDB_ENTITY=hamelsmu WANDB_PROJECT=deepspeed-bench WANDB_LOG_MODEL=all WANDB_RUN_ID=z3-3gpu-v4 \\\n",
    "    torchrun --nproc_per_node 3 run_lora.py \\\n",
    "  --model_id {model_id} \\\n",
    "  --dataset_path dolly-processed \\\n",
    "  --output_dir {model_id}-fa \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --learning_rate 4e-3 \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --bf16 True \\\n",
    "  --tf32 True \\\n",
    "  --lr_scheduler_type \"constant_with_warmup\" \\\n",
    "  --logging_steps 25 \\\n",
    "  --save_steps 100 \\\n",
    "  --save_total_limit 3 \\\n",
    "  --report_to \"wandb\" \\\n",
    "  --deepspeed z3.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
