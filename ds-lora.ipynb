{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80ce8a3-4af5-4319-9fd7-3b056f155524",
   "metadata": {},
   "source": [
    "This notebook follows [this blog post](https://www.philschmid.de/deepspeed-lora-flash-attention) from Phillip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c3359-96fc-4cf8-bae1-cb853d8d45f0",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1346b76-15b4-44f9-ba2a-52a1709e22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d695f99-f0f7-4d42-a2b9-bdd02e2e2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_ENTITY\"] = \"hamelsmu\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"deepspeed-bench\" # log to your project \n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"all\" # log your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a829c650-4b3b-4a70-a628-072fa125bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'From the passage mention the years in which Virat Kohli won a trophy. Display the results in comma separated format.', 'context': \"Virat Kohli (Hindi pronunciation: [ʋɪˈɾɑːʈ ˈkoːɦli] (listen); born 5 November 1988) is an Indian international cricketer and the former captain of the Indian national cricket team who plays as a right-handed batsman for Royal Challengers Bangalore in the IPL and for the Delhi in Indian domestic cricket. Widely regarded as one of the greatest batsmen of all time, Kohli holds the records for scoring most runs in T20 internationals and in the IPL. In 2020, the International Cricket Council named him the male cricketer of the decade. Kohli has also contributed to a number of India's successes, including winning the 2011 World Cup and the 2013 Champions trophy.\\n\\nBorn and raised in New Delhi, Kohli trained at the West Delhi Cricket Academy and started his youth career with the Delhi Under-15 team. He made his international debut in 2008 and quickly became a key player in the ODI team and later made his Test debut in 2011. In 2013, Kohli reached the number one spot in the ICC rankings for ODI batsmen for the first time. During 2014 T20 World Cup, he set a record for the most runs scored in the tournament. In 2018, he achieved yet another milestone, becoming the world's top-ranked Test batsman, making him the only Indian cricketer to hold the number one spot in all three formats of the game. His form continued in 2019, when he became the first player to score 20,000 international runs in a single decade. In 2021, Kohli made the decision to step down as the captain of the Indian national team for T20Is, following the T20 World Cup and in early 2022 he stepped down as the captain of the Test team as well.\", 'response': '2011, 2013', 'category': 'information_extraction'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", \n",
    "                       split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce68baeb-d0e6-45d4-a7ac-1cce6680fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b52bf26-6e74-4ac1-8b8c-8015d1d72619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is the best tv series in the world\n",
      "\n",
      "### Answer\n",
      "Dexter- The Dexter is so exciting to watch that it should be the best TV series in the world\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a58065-1b1a-4eab-b2dc-6d3debb45be5",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29f1eb1d-e57a-4903-a68b-9018c5118ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd9b5854-3513-4a79-964f-ab68f93670d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863e212f-0c29-420a-8100-f1405d14ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What's the best career advise you could give a new graduate entering into a technical or analytics field?\n",
      "\n",
      "### Answer\n",
      "A key feature of any technical or analytics job is that you will be constantly challenged with new trying something new.  You will need to learn to learn, i.e. to teach yourself new techniques and new technology, and to have the courage to apply these things to complex challenges where it's not always clear that you have the right answer (and where very few if any people can fully validate your work).  For many people, the uncertainty and risk of failure associated with these situations tends to push them towards safe and known opportunities with well-tread paths for them to follow.  If you crave a fulfilling career, have the courage to pursue the paths less traveled, acknowledge and learn from your failures and surround yourself with others similarly willing to stretch and grow. You will find such choices lead to a more satisfying career over the longer term.</s>\n"
     ]
    }
   ],
   "source": [
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eabe0d1-f593-4c94-a05e-c51dce3320ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf7cd2-017c-4ea7-ad3e-08c9fd9971f2",
   "metadata": {},
   "source": [
    "I changed `chunk_length` below to get the desired sequence length I wanted.  I had two versions: \n",
    "\n",
    "1. this `2048` version from Phillips blog post, which is named `dolly-processed`\n",
    "2. and a really small one that was `64`, which I cut off at `3200` examples for to keep it extra small.  I named this data `dolly-processed-tiny-truncated`.  I used this small version for the situation where I was comparing bs=1 with bs=200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "054cc4de-fe38-402f-aa51-a86c6a1f0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4bc7b19-8320-41a8-b4f8-72e844897de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 15011/15011 [00:01<00:00, 7729.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721b5d9-e8cd-4b25-a91c-4c39e17b7582",
   "metadata": {},
   "source": [
    "### What is going on here?\n",
    "\n",
    "`chunk` is packing examples into one contiguous \"row\" that is of sequence length=2048, the remainder get's put into the next \"batch\" such that there is effectively no padding.  Basically we are cramming as much data through the model as possible.  \n",
    "\n",
    "I can imagine this is how pre-training works more generally, but I am not sure this is something that makes sense for instruction-tuning?  The reason is that in practice the model isn't going to see examples that are like this, so should we really be instruction tuning this way?  (I DM'd Phillip on Twitter to ask him his opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "211da60f-2738-41c3-a618-121d730fc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq len of 1st example: 2048\n"
     ]
    }
   ],
   "source": [
    "print(f\"seq len of 1st example: {len(lm_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc6b972d-cc29-413f-89c2-ee0b4d751db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq len of 2nd example: 2048\n"
     ]
    }
   ],
   "source": [
    "print(f\"seq len of 2nd example: {len(lm_dataset[1]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcdeef09-cbe5-46d4-8a51-fea108624b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27ca72-ad70-4511-bb7e-2f131001f6bd",
   "metadata": {},
   "source": [
    "### Save Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f07278c-b490-41a3-bf42-085f06c6f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████| 1581/1581 [00:00<00:00, 28107.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset.save_to_disk('dolly-processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbd1f2-dbaa-4e49-b08c-173707d33476",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Look at the overview tab of runs in [this project](https://wandb.ai/hamelsmu/deepspeed-bench?workspace=user-hamelsmu) to see (most) of the CLI command used to run each go of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "289638cf-f8bb-401f-8b12-c227ec717567",
   "metadata": {},
   "outputs": [],
   "source": [
    "!WANDB_ENTITY=hamelsmu WANDB_PROJECT=deepspeed-bench WANDB_LOG_MODEL=all WANDB_RUN_ID=z3-3gpu-v4 \\\n",
    "    torchrun --nproc_per_node 3 run_lora.py \\\n",
    "  --model_id {model_id} \\\n",
    "  --dataset_path dolly-processed \\\n",
    "  --output_dir {model_id}-fa \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --learning_rate 4e-3 \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --bf16 True \\\n",
    "  --tf32 True \\\n",
    "  --lr_scheduler_type \"constant_with_warmup\" \\\n",
    "  --logging_steps 25 \\\n",
    "  --save_steps 100 \\\n",
    "  --save_total_limit 3 \\\n",
    "  --report_to \"wandb\" \\\n",
    "  --deepspeed z3.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
