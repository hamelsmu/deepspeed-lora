{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328c49e2-ea9e-4e94-abd3-bf88b4eb0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp run_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800091ef-d45f-45f1-a864-d210b9fb2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.script import call_parse, Param, store_true\n",
    "from random import randint\n",
    "import os, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ea139d6-3965-4478-9236-872f64ea246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@call_parse\n",
    "def gen_cli(debug:Param('Print command instead of running it', store_true),\n",
    "            ds_stg:Param('The deepspeed stage', int, choices=[0,1,2,3])=3,\n",
    "            n_gpu:Param('number of GPUs', int, choices=[1,2,3])=1,\n",
    "            gc:Param('Toggle gradient checkpointing', choices=['True', 'False'])='True',  \n",
    "            seq_len:Param('Sequence length', int, choices=[64, 200,512, 1024, 2048])=256,\n",
    "            bs:Param('Batch size', int, choices=[1,3,4,6,8,16,32,64,100,200])=1,\n",
    "            model_sz:Param('Model size in Billions', int, choices=[3, 7, 13, 34])=7,\n",
    "            n_epochs:Param('# of epochs', int) = 1,\n",
    "            qaunt4:Param('Enable 4bit quantization', choices=['True', 'False'])='False'):\n",
    "    \"Generate Training CLI Command\"\n",
    "\n",
    "    model_id = {3:'pankajmathur/orca_mini_3b', 7:'NousResearch/Llama-2-7b-hf', 13: 'NousResearch/Llama-2-13b-hf', 34: 'NousResearch/CodeLlama-34b-hf'}[model_sz]\n",
    "    nr = randint(10000000,99999999)\n",
    "    env_values = [('WANDB_ENTITY', 'hamelsmu'), ('WANDB_PROJECT', 'deepspeed-data'),\n",
    "                  ('WANDB_RUN_ID', f'z{ds_stg}-n_gpu{n_gpu}-gc{gc}-seq_len{seq_len}-bs{bs}-model_sz{model_sz}-quant4{qaunt4}-{nr}')]\n",
    "    env_str = ''\n",
    "    for v in env_values:\n",
    "        env_str  += f'{v[0]}={v[1]} '\n",
    "    \n",
    "    cmd = f\"\"\"torchrun --nproc_per_node {n_gpu} run_lora.py \\\n",
    "  --model_id {model_id} \\\n",
    "  --dataset_path data_{seq_len} \\\n",
    "  --quant4 {qaunt4} \\\n",
    "  --output_dir {model_id}-fa \\\n",
    "  --num_train_epochs {n_epochs} \\\n",
    "  --per_device_train_batch_size {bs} \\\n",
    "  --learning_rate 4e-3 \\\n",
    "  --save_strategy no \\\n",
    "  --gradient_checkpointing {gc} \\\n",
    "  --bf16 True \\\n",
    "  --tf32 True \\\n",
    "  --lr_scheduler_type constant_with_warmup \\\n",
    "  --logging_steps 25 \\\n",
    "  --report_to wandb \\\n",
    "  --deepspeed z{ds_stg}.json\"\"\"\n",
    "\n",
    "    full_cmd = env_str + ' ' + cmd\n",
    "\n",
    "    if debug:\n",
    "        print(full_cmd)\n",
    "    else:\n",
    "        env_vars = os.environ.copy()\n",
    "        for v in env_values:\n",
    "            env_vars[v[0]] = v[1]\n",
    "        print(f\"running command:\\n{full_cmd}\")\n",
    "        return subprocess.run(cmd.split(), env=env_vars, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33740129-9df9-4285-93fd-d995cd7df844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "nb_export('run_bench.ipynb', lib_path='.', name='run_bench')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817a4a83-b1bd-497e-8f87-76a0c73f1a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_bench.py [-h] [--debug] [--ds_stg {0,1,2,3}] [--n_gpu {1,2,3}]\n",
      "                    [--gc {True,False}] [--seq_len {64,200,512,1024,2048}]\n",
      "                    [--bs {1,3,4,6,8,16,32,64,100,200}] [--model_sz {3,7,13,34}]\n",
      "                    [--n_epochs N_EPOCHS] [--qaunt4 {True,False}]\n",
      "\n",
      "Generate Training CLI Command\n",
      "\n",
      "options:\n",
      "  -h, --help                         show this help message and exit\n",
      "  --debug                            Print command instead of running it\n",
      "                                     (default: False)\n",
      "  --ds_stg {0,1,2,3}                 The deepspeed stage (default: 3)\n",
      "  --n_gpu {1,2,3}                    number of GPUs (default: 1)\n",
      "  --gc {True,False}                  Toggle gradient checkpointing (default:\n",
      "                                     True)\n",
      "  --seq_len {64,200,512,1024,2048}   Sequence length (default: 256)\n",
      "  --bs {1,3,4,6,8,16,32,64,100,200}  Batch size (default: 1)\n",
      "  --model_sz {3,7,13,34}             Model size in Billions (default: 7)\n",
      "  --n_epochs N_EPOCHS                # of epochs (default: 1)\n",
      "  --qaunt4 {True,False}              Enable 4bit quantization (default: False)\n"
     ]
    }
   ],
   "source": [
    "!python run_bench.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0b3eb7-c4a8-403c-b1ac-8ab294ade0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_ENTITY=hamelsmu WANDB_PROJECT=deepspeed-data WANDB_RUN_ID=z0-n_gpu1-gcTrue-seq_len64-bs1-model_sz13-quant4False-59016160  torchrun --nproc_per_node 1 run_lora.py   --model_id NousResearch/Llama-2-13b-hf   --dataset_path data_64   --quant4 False   --output_dir NousResearch/Llama-2-13b-hf-fa   --num_train_epochs 1   --per_device_train_batch_size 1   --learning_rate 4e-3   --save_strategy no   --gradient_checkpointing True   --bf16 True   --tf32 True   --lr_scheduler_type constant_with_warmup   --logging_steps 25   --report_to wandb   --deepspeed z0.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python run_bench.py --ds_stg 0 --n_gpu 1 --gc True \\\n",
    "--seq_len 64 --bs 1 --model_sz 13 --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b8dd6d7-2d35-473c-9090-85d0052127ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_ENTITY=hamelsmu WANDB_PROJECT=deepspeed-data WANDB_RUN_ID=z0-n_gpu1-gcFalse-seq_len512-bs1-model_sz7-quant4True-83868131  torchrun --nproc_per_node 1 run_lora.py   --model_id NousResearch/Llama-2-7b-hf   --dataset_path data_512   --quant4 True   --output_dir NousResearch/Llama-2-7b-hf-fa   --num_train_epochs 1   --per_device_train_batch_size 1   --learning_rate 4e-3   --save_strategy no   --gradient_checkpointing False   --bf16 True   --tf32 True   --lr_scheduler_type constant_with_warmup   --logging_steps 25   --report_to wandb   --deepspeed z0.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# z0-n_gpu1-gcFalse-seq_len512-bs1-model_sz7\n",
    "\n",
    "python run_bench.py --ds_stg 0 --n_gpu 1 --gc False \\\n",
    "--seq_len 512 --bs 1 --model_sz 7 --qaunt4 True --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b67504a9-64e1-47bd-80c1-660083ad3bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_ENTITY=hamelsmu WANDB_PROJECT=deepspeed-data WANDB_RUN_ID=z0-n_gpu1-gcTrue-seq_len64-bs1-model_sz7-quant4True-88368670  torchrun --nproc_per_node 1 run_lora.py   --model_id NousResearch/Llama-2-7b-hf   --dataset_path data_64   --quant4 True   --output_dir NousResearch/Llama-2-7b-hf-fa   --num_train_epochs 1   --per_device_train_batch_size 1   --learning_rate 4e-3   --save_strategy no   --gradient_checkpointing True   --bf16 True   --tf32 True   --lr_scheduler_type constant_with_warmup   --logging_steps 25   --report_to wandb   --deepspeed z0.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# z0-n_gpu1-gcFalse-seq_len64-bs1-model_sz7-quant4True-36346237\n",
    "\n",
    "python run_bench.py --ds_stg 0 --n_gpu 1 --gc True \\\n",
    "--seq_len 64 --bs 1 --model_sz 7 --qaunt4 True --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7843-72c7-4d96-b9bb-1fd32f512ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
